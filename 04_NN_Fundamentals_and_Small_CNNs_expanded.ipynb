{"cells":[{"cell_type":"markdown","id":"25a71b68","metadata":{"id":"25a71b68"},"source":["# Module 4 — Neural Network Fundamentals & Small CNNs (Expanded)\n","\n","This notebook teaches:\n","\n","- basic MLP on MNIST (Keras)\n","- small CNN on CIFAR-10 (Keras)\n","- equivalent small CNN training loop in PyTorch\n","- visualizing filters and feature maps\n","- saving and loading models\n","\n","Designed for classroom demos: cells run quickly (few epochs)."]},{"cell_type":"markdown","id":"24edff96","metadata":{"id":"24edff96"},"source":["## 1 — Setup (install packages and imports)"]},{"cell_type":"code","execution_count":1,"id":"3e1b228e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3e1b228e","executionInfo":{"status":"ok","timestamp":1761074970094,"user_tz":-330,"elapsed":163711,"user":{"displayName":"Dr. Prashant Pawar","userId":"04717019021953533355"}},"outputId":"5379116c-fd75-44e9-ce3a-beb4171dd7e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m157.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m159.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.9.0 which is incompatible.\n","fastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mTF version: 2.19.0\n","Torch version: 2.9.0+cu128\n"]}],"source":["# Install necessary packages\n","!pip -q install -U torch torchvision matplotlib --quiet\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","print('TF version:', tf.__version__)\n","import torch\n","import torchvision\n","print('Torch version:', torch.__version__)\n","\n","# fix random seeds for reproducibility\n","np.random.seed(42)\n","import random\n","random.seed(42)\n","import os\n","os.environ['PYTHONHASHSEED'] = '42'\n"]},{"cell_type":"markdown","id":"856d110f","metadata":{"id":"856d110f"},"source":["## 2 — MLP on MNIST (Keras) — quick demo"]},{"cell_type":"code","execution_count":null,"id":"f58c0c6d","metadata":{"id":"f58c0c6d"},"outputs":[],"source":["from tensorflow.keras.datasets import mnist\n","(x_train,y_train),(x_test,y_test)=mnist.load_data()\n","# use small subset for demo speed\n","x_train = x_train[:10000].reshape(-1,28*28)/255.0\n","y_train = y_train[:10000]\n","x_test = x_test[:2000].reshape(-1,28*28)/255.0\n","\n","def build_mlp():\n","    model = models.Sequential([\n","        layers.Input(28*28),\n","        layers.Dense(256, activation='relu'),\n","        layers.Dropout(0.3),\n","        layers.Dense(128, activation='relu'),\n","        layers.Dense(10, activation='softmax')\n","    ])\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","mlp = build_mlp()\n","mlp.summary()\n","history = mlp.fit(x_train, y_train, epochs=5, validation_split=0.1, batch_size=128)\n","plt.plot(history.history['accuracy'], label='train_acc')\n","plt.plot(history.history['val_accuracy'], label='val_acc')\n","plt.legend(); plt.title('MLP accuracy')\n"]},{"cell_type":"markdown","id":"aee77ac8","metadata":{"id":"aee77ac8"},"source":["## 3 — Small CNN on CIFAR-10 (Keras)"]},{"cell_type":"code","execution_count":null,"id":"d6f9f750","metadata":{"id":"d6f9f750"},"outputs":[],"source":["from tensorflow.keras.datasets import cifar10\n","(x_train,y_train),(x_test,y_test)=cifar10.load_data()\n","# use subset for demo speed\n","x_train = x_train[:10000].astype('float32')/255.0\n","y_train = y_train[:10000]\n","x_test = x_test[:2000].astype('float32')/255.0\n","\n","def build_small_cnn():\n","    model = models.Sequential([\n","        layers.Input(shape=(32,32,3)),\n","        layers.Conv2D(32,3,activation='relu', padding='same'),\n","        layers.Conv2D(32,3,activation='relu', padding='same'),\n","        layers.MaxPooling2D(),\n","        layers.Dropout(0.25),\n","        layers.Conv2D(64,3,activation='relu', padding='same'),\n","        layers.Conv2D(64,3,activation='relu', padding='same'),\n","        layers.MaxPooling2D(),\n","        layers.Flatten(),\n","        layers.Dense(128, activation='relu'),\n","        layers.Dropout(0.5),\n","        layers.Dense(10, activation='softmax')\n","    ])\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","cnn = build_small_cnn()\n","cnn.summary()\n","history = cnn.fit(x_train, y_train, epochs=6, batch_size=128, validation_split=0.1)\n","plt.figure(figsize=(10,4))\n","plt.subplot(1,2,1); plt.plot(history.history['loss'], label='train_loss'); plt.plot(history.history['val_loss'], label='val_loss'); plt.legend()\n","plt.subplot(1,2,2); plt.plot(history.history['accuracy'], label='train_acc'); plt.plot(history.history['val_accuracy'], label='val_acc'); plt.legend()\n","plt.show()\n","\n","# Evaluate\n","print('Evaluate on test subset:')\n","print(cnn.evaluate(x_test, y_test[:len(x_test)]))\n"]},{"cell_type":"markdown","id":"4099e09e","metadata":{"id":"4099e09e"},"source":["## 4 — Visualize filters and feature maps (Keras)"]},{"cell_type":"code","execution_count":null,"id":"43530787","metadata":{"id":"43530787"},"outputs":[],"source":["# Visualize first conv layer filters\n","first_conv = cnn.layers[0]\n","weights = first_conv.get_weights()[0]  # shape (3,3,3,32)\n","print('Weights shape:', weights.shape)\n","\n","# Normalize and plot first 8 filters\n","fig, axes = plt.subplots(1,8, figsize=(12,3))\n","for i in range(8):\n","    f = weights[:,:,:,i]\n","    f_min, f_max = f.min(), f.max()\n","    f_img = (f - f_min) / (f_max - f_min)\n","    axes[i].imshow(f_img)\n","    axes[i].axis('off')\n","plt.suptitle('First conv filters (visualized)')\n","plt.show()\n","\n","# Feature map output for one image\n","from tensorflow.keras import Model\n","img = x_test[0:1]\n","layer_outputs = [layer.output for layer in cnn.layers if 'conv' in layer.name]\n","activation_model = Model(inputs=cnn.input, outputs=layer_outputs)\n","activations = activation_model.predict(img)\n","print('Number of conv layers activations:', len(activations))\n","# show first 6 feature maps of first conv layer\n","act = activations[0]\n","fig, axes = plt.subplots(1,6, figsize=(12,2))\n","for i in range(6):\n","    axes[i].imshow(act[0,:,:,i], cmap='viridis')\n","    axes[i].axis('off')\n","plt.suptitle('Feature maps from first conv layer')\n","plt.show()\n"]},{"cell_type":"markdown","id":"ff7deed3","metadata":{"id":"ff7deed3"},"source":["## 5 — Small CNN training loop in PyTorch (quick demo)"]},{"cell_type":"code","execution_count":null,"id":"efd58506","metadata":{"id":"efd58506"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Subset\n","\n","# Prepare CIFAR-10 with torchvision\n","transform = transforms.Compose([transforms.ToTensor()])\n","trainset = datasets.CIFAR10(root='/tmp/cifar', train=True, download=True, transform=transform)\n","testset = datasets.CIFAR10(root='/tmp/cifar', train=False, download=True, transform=transform)\n","# use small subsets\n","train_subset = Subset(trainset, range(2000))\n","test_subset = Subset(testset, range(500))\n","train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)\n","\n","class SmallCNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(3,32,3,padding=1), nn.ReLU(),\n","            nn.Conv2d(32,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n","            nn.Conv2d(32,64,3,padding=1), nn.ReLU(), nn.Conv2d(64,64,3,padding=1), nn.ReLU(), nn.MaxPool2d(2)\n","        )\n","        self.fc = nn.Sequential(nn.Flatten(), nn.Linear(64*8*8, 128), nn.ReLU(), nn.Linear(128,10))\n","    def forward(self,x):\n","        x = self.conv(x)\n","        x = self.fc(x)\n","        return x\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model_t = SmallCNN().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model_t.parameters(), lr=1e-3)\n","\n","# training loop\n","for epoch in range(3):\n","    model_t.train()\n","    total_loss = 0\n","    for imgs, labels in train_loader:\n","        imgs, labels = imgs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model_t(imgs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    print(f'Epoch {epoch+1} train loss:', total_loss/len(train_loader))\n","\n","# evaluation\n","model_t.eval()\n","correct = 0; total=0\n","with torch.no_grad():\n","    for imgs, labels in test_loader:\n","        imgs, labels = imgs.to(device), labels.to(device)\n","        outputs = model_t(imgs)\n","        preds = outputs.argmax(dim=1)\n","        correct += (preds==labels).sum().item()\n","        total += labels.size(0)\n","print('PyTorch test accuracy (subset):', correct/total)\n"]},{"cell_type":"markdown","id":"bbc4979c","metadata":{"id":"bbc4979c"},"source":["## 6 — Save models (Keras & PyTorch)"]},{"cell_type":"code","execution_count":null,"id":"c03f6904","metadata":{"id":"c03f6904"},"outputs":[],"source":["# Save Keras model\n","cnn.save('/mnt/data/keras_small_cnn.h5')\n","print('Saved Keras model to /mnt/data/keras_small_cnn.h5')\n","\n","# Save PyTorch model state dict\n","torch.save(model_t.state_dict(), '/mnt/data/torch_small_cnn.pth')\n","print('Saved PyTorch model to /mnt/data/torch_small_cnn.pth')\n"]},{"cell_type":"markdown","id":"f5888572","metadata":{"id":"f5888572"},"source":["## 7 — Exercises & Instructor Notes\n","\n","- Compare MLP vs CNN on visual tasks: observe accuracy differences.\n","- Increase dataset size and epochs for better results; use data augmentation.\n","- Visualize misclassified images and discuss potential causes (label noise, insufficient capacity).\n","- Optional: convert Keras model to TFLite for edge demo."]}],"metadata":{"colab":{"provenance":[],"gpuType":"L4"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}