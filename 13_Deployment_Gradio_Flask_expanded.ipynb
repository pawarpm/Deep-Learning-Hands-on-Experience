{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15884b7",
   "metadata": {},
   "source": [
    "# Module 13 — Deployment & Interactive Demos (Gradio, Flask, TFLite) — Expanded\\n\\nGenerated expanded notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b64b211",
   "metadata": {},
   "source": [
    "# Module 13 — Deployment & Interactive Demos (Expanded)\n",
    "\n",
    "This notebook shows how to export models and build simple Gradio and Flask demos, plus convert a Keras model to TFLite for edge inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a9bce4",
   "metadata": {},
   "source": [
    "## 1 — Setup (install packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U gradio flask tensorflow --quiet\n",
    "import os, numpy as np, tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "print('TF', tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d366a06",
   "metadata": {},
   "source": [
    "## 2 — Create or load a small Keras model (MNIST quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09141b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "# small model for demo\n",
    "mnist_model = models.Sequential([layers.Input((28,28,1)), layers.Conv2D(16,3,activation='relu'), layers.MaxPooling2D(), layers.Flatten(), layers.Dense(10, activation='softmax')])\n",
    "mnist_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# create random weights (or train quickly)\n",
    "import numpy as np\n",
    "x_dummy = np.random.rand(100,28,28,1).astype('float32')\n",
    "y_dummy = np.random.randint(0,10,size=(100,))\n",
    "mnist_model.fit(x_dummy, y_dummy, epochs=1, batch_size=16, verbose=0)\n",
    "mnist_model.save('/mnt/data/demo_mnist_model.h5')\n",
    "print('Saved demo model to /mnt/data/demo_mnist_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e48de04",
   "metadata": {},
   "source": [
    "## 3 — Gradio demo (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a20736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "# load model\n",
    "model = tf.keras.models.load_model('/mnt/data/demo_mnist_model.h5')\n",
    "\n",
    "def predict_fn(img: Image):\n",
    "    img = img.convert('L').resize((28,28))\n",
    "    arr = np.array(img).astype('float32')/255.0\n",
    "    arr = arr.reshape(1,28,28,1)\n",
    "    preds = model.predict(arr)\n",
    "    label = int(preds.argmax())\n",
    "    return label\n",
    "\n",
    "demo = gr.Interface(fn=predict_fn, inputs=gr.Image(type='pil'), outputs='label')\n",
    "# NOTE: demo.launch() will open a local server; in Colab set share=True\n",
    "# demo.launch(share=True)\n",
    "print('Gradio demo ready. In Colab, run demo.launch(share=True) to get a public link.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5ddf8d",
   "metadata": {},
   "source": [
    "## 4 — Minimal Flask inference endpoint (example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c62310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Flask app is for local deployment (not run in hosted notebook UI)\n",
    "from flask import Flask, request, jsonify\n",
    "app = Flask('demo_app')\n",
    "model_flask = tf.keras.models.load_model('/mnt/data/demo_mnist_model.h5')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # expects JSON with 'image' as base64 or other scheme; here we return dummy\n",
    "    return jsonify({'label': int(0), 'confidence': float(1.0)})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "print('Flask example included. To run the Flask server, run this cell as a script in a terminal.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653cd477",
   "metadata": {},
   "source": [
    "## 5 — Convert to TFLite (Keras -> TFLite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert demo Keras model to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(mnist_model)\n",
    "tflite_model = converter.convert()\n",
    "tflite_path = '/mnt/data/demo_mnist_model.tflite'\n",
    "open(tflite_path, 'wb').write(tflite_model)\n",
    "print('Saved TFLite model to', tflite_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded0151",
   "metadata": {},
   "source": [
    "## 6 — Quick tips for classroom: hosting, sharing, and demos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f069133e",
   "metadata": {},
   "source": [
    "- Use Gradio `share=True` in Colab to get a temporary public link for demos.\n",
    "- Pre-run heavy training and save model weights; in class load weights and show inference quickly.\n",
    "- For production, containerize Flask app and expose via a cloud VM or serverless function.\n",
    "- For edge demos, use TFLite and run inference on device (Raspberry Pi, Android)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
