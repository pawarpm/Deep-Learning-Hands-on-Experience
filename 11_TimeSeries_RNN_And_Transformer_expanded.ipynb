{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d4d12e",
   "metadata": {},
   "source": [
    "# Module 11 — Time Series & Signal Modeling (Expanded)\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "- creating a synthetic NDVI-like time series dataset\n",
    "- feature engineering (lags, rolling stats)\n",
    "- sliding-window data preparation\n",
    "- LSTM forecasting (Keras)\n",
    "- simple Transformer-style forecaster using Keras `MultiHeadAttention`\n",
    "- evaluation metrics (MAE, MAPE) and baseline comparison\n",
    "- saving models\n",
    "\n",
    "Designed for classroom demos: uses small synthetic data and short training runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9497a9",
   "metadata": {},
   "source": [
    "## 1 — Setup (install packages and imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e02f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U tensorflow==2.12.0 numpy pandas matplotlib scikit-learn --quiet\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print('TF version:', tf.__version__)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4340c8",
   "metadata": {},
   "source": [
    "## 2 — Create synthetic NDVI-like time series and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic NDVI time series with seasonality + trend + noise\n",
    "periods = 60  # months (5 years)\n",
    "dates = pd.date_range('2018-01-01', periods=periods, freq='M')\n",
    "trend = 0.01 * np.arange(periods)\n",
    "seasonal = 0.2 * np.sin(np.linspace(0, 6*np.pi, periods))\n",
    "noise = np.random.normal(0, 0.03, periods)\n",
    "ndvi = 0.4 + trend + seasonal + noise\n",
    "\n",
    "df = pd.DataFrame({'date': dates, 'ndvi': ndvi})\n",
    "df.set_index('date', inplace=True)\n",
    "df['ndvi'].plot(figsize=(10,3), title='Synthetic NDVI time series'); plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5644ed1d",
   "metadata": {},
   "source": [
    "## 3 — Feature engineering: lags and rolling statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339c3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = df.copy()\n",
    "for lag in [1,2,3,6,12]:\n",
    "    df_feat[f'lag_{lag}'] = df_feat['ndvi'].shift(lag)\n",
    "# rolling mean\n",
    "df_feat['roll_mean_3'] = df_feat['ndvi'].rolling(3).mean()\n",
    "df_feat = df_feat.dropna()\n",
    "print('Feature frame shape:', df_feat.shape)\n",
    "df_feat.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d967cf0",
   "metadata": {},
   "source": [
    "## 4 — Prepare sliding-window arrays for supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03beff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll prepare windows: use past `window_size` values to predict next value\n",
    "values = df['ndvi'].values\n",
    "window_size = 12\n",
    "X, y = [], []\n",
    "for i in range(len(values)-window_size):\n",
    "    X.append(values[i:i+window_size])\n",
    "    y.append(values[i+window_size])\n",
    "X = np.array(X)  # shape (samples, timesteps)\n",
    "y = np.array(y)\n",
    "print('X shape', X.shape, 'y shape', y.shape)\n",
    "\n",
    "# train/val/test split (no shuffle - time series)\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# scale using MinMax on training data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_flat = scaler.fit_transform(X_train)\n",
    "X_train = X_train_flat.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "# apply same scaler to val/test (reshape to 2D then back)\n",
    "X_val = scaler.transform(X_val).reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "X_test = scaler.transform(X_test).reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# scale y using same scaler fitted on flattened sequences target? Simpler: scale by overall min/max of train sequences' range\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1,1)).flatten()\n",
    "y_val_scaled = y_scaler.transform(y_val.reshape(-1,1)).flatten()\n",
    "y_test_scaled = y_scaler.transform(y_test.reshape(-1,1)).flatten()\n",
    "\n",
    "print('Shapes after reshape:', X_train.shape, X_val.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2037fed",
   "metadata": {},
   "source": [
    "## 5 — Baseline: persistence model and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e56170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(true, pred):\n",
    "    return np.mean(np.abs((true - pred) / (true + 1e-8))) * 100\n",
    "\n",
    "# persistence baseline: predict last value in window\n",
    "persistence_pred = X_test[:, -1, 0]\n",
    "print('Persistence MAPE:', mape(y_test, persistence_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a12cf22",
   "metadata": {},
   "source": [
    "## 6 — LSTM forecasting model (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e0b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "model_lstm = models.Sequential([\n",
    "    layers.Input(shape=(window_size,1)),\n",
    "    layers.LSTM(64, return_sequences=False),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "model_lstm.compile(optimizer='adam', loss='mse')\n",
    "model_lstm.summary()\n",
    "\n",
    "history = model_lstm.fit(X_train, y_train_scaled, validation_data=(X_val, y_val_scaled), epochs=40, batch_size=8, verbose=0)\n",
    "\n",
    "# plot loss\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend(); plt.title('LSTM training loss')\n",
    "plt.show()\n",
    "\n",
    "# predict and invert scaling\n",
    "pred_lstm_scaled = model_lstm.predict(X_test).flatten()\n",
    "pred_lstm = y_scaler.inverse_transform(pred_lstm_scaled.reshape(-1,1)).flatten()\n",
    "print('LSTM Test MAPE:', mape(y_test, pred_lstm))\n",
    "print('LSTM Test MAE:', mean_absolute_error(y_test, pred_lstm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19865047",
   "metadata": {},
   "source": [
    "## 7 — Simple Transformer-style model (Keras MultiHeadAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973332b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Positional encoding\n",
    "def add_positional_encoding(x):\n",
    "    # x shape: (batch, timesteps, features)\n",
    "    timesteps = tf.shape(x)[1]\n",
    "    pos = tf.cast(tf.range(timesteps), tf.float32)[..., tf.newaxis]\n",
    "    return x + pos * 1e-3  # small positional bias (simple)\n",
    "\n",
    "inputs = layers.Input(shape=(window_size,1))\n",
    "# project to higher dimension\n",
    "x = layers.Dense(64)(inputs)\n",
    "x = add_positional_encoding(x)\n",
    "# Multi-head attention\n",
    "attn_output = layers.MultiHeadAttention(num_heads=4, key_dim=16)(x, x)\n",
    "x = layers.Add()([x, attn_output])\n",
    "x = layers.LayerNormalization()(x)\n",
    "# global pooling and output\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model_trans = models.Model(inputs, outputs)\n",
    "model_trans.compile(optimizer='adam', loss='mse')\n",
    "model_trans.summary()\n",
    "\n",
    "history_t = model_trans.fit(X_train, y_train_scaled, validation_data=(X_val, y_val_scaled), epochs=30, batch_size=8, verbose=0)\n",
    "\n",
    "pred_t_scaled = model_trans.predict(X_test).flatten()\n",
    "pred_t = y_scaler.inverse_transform(pred_t_scaled.reshape(-1,1)).flatten()\n",
    "print('Transformer-style Test MAPE:', mape(y_test, pred_t))\n",
    "print('Transformer-style Test MAE:', mean_absolute_error(y_test, pred_t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183517ab",
   "metadata": {},
   "source": [
    "## 8 — Visualize forecasts vs actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b2db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(range(len(y_test)), y_test, label='actual')\n",
    "plt.plot(range(len(pred_lstm)), pred_lstm, label='LSTM_pred')\n",
    "plt.plot(range(len(pred_t)), pred_t, label='Transformer_pred')\n",
    "plt.legend(); plt.title('Forecasts vs Actuals (Test set)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d3ca6",
   "metadata": {},
   "source": [
    "## 9 — Save models and instructor notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce7ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.save('/mnt/data/lstm_forecast.h5')\n",
    "model_trans.save('/mnt/data/transformer_forecast.h5')\n",
    "print('Saved models to /mnt/data')\n",
    "\n",
    "# Notes for instructors:\n",
    "# - Use real NDVI or other time series for better demonstration.\n",
    "# - For longer sequences, consider using sliding windows with stride and batching.\n",
    "# - Discuss proper cross-validation for time series (walk-forward validation).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
