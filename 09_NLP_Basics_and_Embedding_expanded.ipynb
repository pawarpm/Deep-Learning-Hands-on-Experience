{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35849a04",
   "metadata": {},
   "source": [
    "# Module 9 — NLP Basics & Embeddings (Expanded)\n",
    "\n",
    "This notebook covers practical, classroom-friendly NLP workflows:\n",
    "\n",
    "- text preprocessing and TF-IDF baseline\n",
    "- LSTM-based classifier (Keras) on IMDB (small subset)\n",
    "- Hugging Face tokenization and feature extraction (DistilBERT)\n",
    "- fine-tuning DistilBERT for text classification (short demo)\n",
    "\n",
    "Notes: run in Colab with a GPU for transformer fine-tuning. Training steps are intentionally short for demos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c466ba7",
   "metadata": {},
   "source": [
    "## 1 — Setup (install packages and imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab8745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries (transformers and datasets are optional but useful for HF experiments)\n",
    "!pip -q install -U transformers datasets sentencepiece --quiet\n",
    "!pip -q install -U tensorflow scikit-learn --quiet\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print('TF version:', tf.__version__)\n",
    "print('Transformers available')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ae153e",
   "metadata": {},
   "source": [
    "## 2 — TF-IDF Baseline (small synthetic dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb867086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small synthetic dataset for quick TF-IDF demo\n",
    "docs = [\n",
    "    'I love this movie, it is fantastic and exciting',\n",
    "    'This film was terrible and boring',\n",
    "    'What a great and thrilling performance',\n",
    "    'I did not like the movie, it was dull',\n",
    "    'Amazing acting and wonderful score',\n",
    "    'Bad plot and poor acting'\n",
    "]\n",
    "labels = [1,0,1,0,1,0]  # 1=positive, 0=negative\n",
    "\n",
    "v = TfidfVectorizer(ngram_range=(1,2), max_features=50)\n",
    "X = v.fit_transform(docs)\n",
    "print('TF-IDF shape:', X.shape)\n",
    "\n",
    "# Simple logistic regression baseline\n",
    "clf = LogisticRegression().fit(X, labels)\n",
    "print('Baseline training done. Example preds:', clf.predict(v.transform(['fantastic movie','poor acting'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df551c35",
   "metadata": {},
   "source": [
    "## 3 — LSTM classifier on IMDB (Keras) — quick demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e99f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF Keras IMDB dataset (integer-encoded)\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# limit vocab and sequence length for speed\n",
    "VOCAB_SIZE = 10000\n",
    "MAXLEN = 200\n",
    "(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=VOCAB_SIZE)\n",
    "# use small subset for demo\n",
    "x_train = x_train[:8000]; y_train = y_train[:8000]\n",
    "x_test = x_test[:2000]; y_test = y_test[:2000]\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=MAXLEN)\n",
    "x_test = pad_sequences(x_test, maxlen=MAXLEN)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(VOCAB_SIZE, 64, input_length=MAXLEN),\n",
    "    layers.Bidirectional(layers.LSTM(64)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split=0.1, epochs=3, batch_size=64)\n",
    "plt.plot(history.history['accuracy'], label='train_acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "plt.legend(); plt.title('LSTM accuracy')\n",
    "\n",
    "# Evaluate\n",
    "print('Eval:', model.evaluate(x_test, y_test, verbose=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4640bf0",
   "metadata": {},
   "source": [
    "## 4 — Hugging Face tokenization and feature extraction (DistilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "encoder = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "# sample texts\n",
    "texts = ['I love this movie', 'This movie is awful']\n",
    "enc = tokenizer(texts, padding=True, truncation=True, return_tensors='tf')\n",
    "outputs = encoder(enc)\n",
    "# pooled output example: take mean of last hidden states\n",
    "embeds = tf.reduce_mean(outputs.last_hidden_state, axis=1)\n",
    "print('Embeddings shape:', embeds.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8288ff6e",
   "metadata": {},
   "source": [
    "## 5 — Fine-tune DistilBERT for text classification (demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb708db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "# Use a very small dataset for demo (reuse TF-IDF docs above)\n",
    "texts = docs\n",
    "labels = np.array(labels)\n",
    "enc = tokenizer(texts, padding=True, truncation=True, return_tensors='tf')\n",
    "\n",
    "model_tf = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# compile with TF optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model_tf.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# Create a tf.data dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(enc), labels)).batch(2)\n",
    "# train briefly (this is a tiny toy example)\n",
    "model_tf.fit(dataset, epochs=1)\n",
    "\n",
    "# Predict\n",
    "pred = model_tf.predict(dict(enc))\n",
    "print('Logits shape:', pred.logits.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b55ec7",
   "metadata": {},
   "source": [
    "## 6 — Save/export models and tips\n",
    "\n",
    "- Save Keras models using `model.save('path')`.\n",
    "- For Hugging Face TF models, use `model_tf.save_pretrained('dir')` and `tokenizer.save_pretrained('dir')`.\n",
    "- Exercises: try training on a small subset of AG News or Yelp datasets from `datasets` library and compare TF-IDF, LSTM, and Transformer approaches."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
