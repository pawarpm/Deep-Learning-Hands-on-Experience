{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2fb381c",
   "metadata": {},
   "source": [
    "# Module 10 — Audio Features & Classification (Expanded)\n",
    "\n",
    "This notebook covers a practical classroom-ready audio workflow:\n",
    "\n",
    "- synthesize a tiny audio dataset (two classes: sine vs noise/chirp)\n",
    "- compute mel-spectrograms using `librosa`\n",
    "- build a `tf.data` pipeline that yields spectrogram images\n",
    "- train a small CNN on spectrograms (few epochs for demo)\n",
    "- evaluate, visualize predictions, and save the model\n",
    "\n",
    "Notes: If you have real audio files, replace the synthetic generation step with uploads or Drive-mounted paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3826fb",
   "metadata": {},
   "source": [
    "## 1 — Setup (install packages and imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6357debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U librosa matplotlib numpy tensorflow --quiet\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "print('librosa version:', librosa.__version__)\n",
    "print('TF version:', tf.__version__)\n",
    "\n",
    "os.makedirs('/mnt/data/audio_dataset', exist_ok=True)\n",
    "os.makedirs('/mnt/data/audio_dataset/class_sine', exist_ok=True)\n",
    "os.makedirs('/mnt/data/audio_dataset/class_noise', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad0e16b",
   "metadata": {},
   "source": [
    "## 2 — Synthesize tiny audio dataset (sine wave vs noise/chirp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9712e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "sr = 16000\n",
    "DURATION = 1.0  # seconds\n",
    "\n",
    "def make_sine(freq, path):\n",
    "    t = np.linspace(0, DURATION, int(sr*DURATION), endpoint=False)\n",
    "    y = 0.5 * np.sin(2*np.pi*freq*t)\n",
    "    sf.write(path, y, sr)\n",
    "\n",
    "def make_chirp(path):\n",
    "    t = np.linspace(0, DURATION, int(sr*DURATION), endpoint=False)\n",
    "    y = 0.5 * librosa.chirp(fmin=300, fmax=3000, sr=sr, length=len(t))\n",
    "    sf.write(path, y, sr)\n",
    "\n",
    "def make_noise(path):\n",
    "    y = 0.5 * np.random.randn(int(sr*DURATION))\n",
    "    sf.write(path, y, sr)\n",
    "\n",
    "# create files\n",
    "for i in range(40):\n",
    "    make_sine(400 + i*10, f'/mnt/data/audio_dataset/class_sine/sine_{i}.wav')\n",
    "for i in range(40):\n",
    "    if i%2==0:\n",
    "        make_chirp(f'/mnt/data/audio_dataset/class_noise/noise_{i}.wav')\n",
    "    else:\n",
    "        make_noise(f'/mnt/data/audio_dataset/class_noise/noise_{i}.wav')\n",
    "\n",
    "print('Created synthetic audio files:', len(os.listdir('/mnt/data/audio_dataset/class_sine')), len(os.listdir('/mnt/data/audio_dataset/class_noise')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b0c62",
   "metadata": {},
   "source": [
    "## 3 — Compute Mel-spectrograms and visualize examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf36755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_to_mel(path, sr=16000, n_mels=64, n_fft=1024, hop_length=256):\n",
    "    y, _ = librosa.load(path, sr=sr)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    return S_db\n",
    "\n",
    "# visualize a few\n",
    "paths = [f'/mnt/data/audio_dataset/class_sine/sine_0.wav', f'/mnt/data/audio_dataset/class_noise/noise_1.wav']\n",
    "plt.figure(figsize=(10,4))\n",
    "for i, p in enumerate(paths):\n",
    "    S_db = wav_to_mel(p)\n",
    "    plt.subplot(1,2,i+1)\n",
    "    librosa.display.specshow(S_db, sr=sr, hop_length=256, x_axis='time', y_axis='mel')\n",
    "    plt.title(os.path.basename(p))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ee868",
   "metadata": {},
   "source": [
    "## 4 — Build `tf.data` pipeline (on-the-fly mel computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4adbee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# collect file paths and labels\n",
    "sine_files = sorted(glob.glob('/mnt/data/audio_dataset/class_sine/*.wav'))\n",
    "noise_files = sorted(glob.glob('/mnt/data/audio_dataset/class_noise/*.wav'))\n",
    "files = sine_files + noise_files\n",
    "labels = [0]*len(sine_files) + [1]*len(noise_files)\n",
    "\n",
    "# simple train/val split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(files, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "def load_mel(path, label):\n",
    "    path = path.decode('utf-8')\n",
    "    S_db = wav_to_mel(path)\n",
    "    # resize or pad to fixed shape (n_mels x time_bins). We'll ensure time_bins = 63 (approx)\n",
    "    # For simplicity, take S_db and pad/crop to (64, 63)\n",
    "    target_shape = (64, 63)\n",
    "    S = S_db\n",
    "    H, W = S.shape\n",
    "    # normalize to 0-1\n",
    "    S = (S - S.min()) / (S.max() - S.min() + 1e-6)\n",
    "    # pad or crop width\n",
    "    if W < target_shape[1]:\n",
    "        pad_width = target_shape[1] - W\n",
    "        S = np.pad(S, ((0,0),(0,pad_width)), mode='constant')\n",
    "    else:\n",
    "        S = S[:, :target_shape[1]]\n",
    "    S = S.astype('float32')\n",
    "    S = np.expand_dims(S, -1)  # channel\n",
    "    return S, np.int64(label)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def tf_load_mel(path, label):\n",
    "    S, lab = tf.py_function(load_mel, [path, label], [tf.float32, tf.int64])\n",
    "    S.set_shape([64,63,1])\n",
    "    lab.set_shape([])\n",
    "    return S, lab\n",
    "\n",
    "BATCH = 16\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_files, train_labels))\n",
    "train_ds = train_ds.shuffle(100).map(tf_load_mel, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_files, val_labels)).map(tf_load_mel, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# inspect\n",
    "for X, y in train_ds.take(1):\n",
    "    print('Batch X shape:', X.shape, 'y shape:', y.shape)\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.imshow(X[0,:,:,0], aspect='auto')\n",
    "    plt.title('Sample mel (normalized)')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f7d9a",
   "metadata": {},
   "source": [
    "## 5 — Build small CNN for spectrogram classification (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45134473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_spec_cnn(input_shape=(64,63,1), num_classes=2):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv2D(16,3,activation='relu', padding='same'), layers.MaxPooling2D((2,2)),\n",
    "        layers.Conv2D(32,3,activation='relu', padding='same'), layers.MaxPooling2D((2,2)),\n",
    "        layers.Conv2D(64,3,activation='relu', padding='same'), layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(64, activation='relu'), layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_spec_cnn()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ca556e",
   "metadata": {},
   "source": [
    "## 6 — Train the CNN (few epochs demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa503983",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 8\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1); plt.plot(history.history['loss'], label='train_loss'); plt.plot(history.history['val_loss'], label='val_loss'); plt.legend(); plt.title('Loss')\n",
    "plt.subplot(1,2,2); plt.plot(history.history['accuracy'], label='train_acc'); plt.plot(history.history['val_accuracy'], label='val_acc'); plt.legend(); plt.title('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc76e179",
   "metadata": {},
   "source": [
    "## 7 — Evaluate and visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c3843",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(val_ds)\n",
    "print('Val loss, acc:', loss, acc)\n",
    "\n",
    "# show some predictions\n",
    "for Xb, yb in val_ds.take(1):\n",
    "    preds = model.predict(Xb)\n",
    "    pred_labels = np.argmax(preds, axis=1)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    for i in range(min(8, Xb.shape[0])):\n",
    "        plt.subplot(2,8,i+1); plt.imshow(Xb[i,:,:,0], aspect='auto'); plt.axis('off'); plt.title(f'T:{yb[i].numpy()} P:{pred_labels[i]}')\n",
    "    plt.suptitle('Validation batch: True vs Predicted')\n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019846f6",
   "metadata": {},
   "source": [
    "## 8 — Save model and tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb82588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/mnt/data/audio_spec_cnn.h5')\n",
    "print('Saved model to /mnt/data/audio_spec_cnn.h5')\n",
    "\n",
    "# Tips: Replace synthetic audio with real dataset (Google Speech Commands, ESC-50, UrbanSound8K).\n",
    "# Use mel-spectrogram augmentation (time-shift, freq masking) for robustness (SpecAugment).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a177d0",
   "metadata": {},
   "source": [
    "## 9 — Exercises & Instructor Notes\n",
    "\n",
    "- Replace synthetic data with a small subset of Speech Commands or ESC-50 for real audio classification.\n",
    "- Implement SpecAugment (time/frequency masking) in the tf.data pipeline and observe robustness gains.\n",
    "- Experiment with 1D CNNs on raw waveform vs 2D CNN on spectrograms and compare performance.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
