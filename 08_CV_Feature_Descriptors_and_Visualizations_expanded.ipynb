{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b1bde1",
   "metadata": {},
   "source": [
    "# Module 8 — Classical & Modern Computer Vision Tasks (Expanded)\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "- edge detection (Canny) and Hough lines\n",
    "- ORB feature detection and descriptor matching (example using synthetic images)\n",
    "- extracting CNN embeddings and visualizing with t-SNE\n",
    "\n",
    "Run cells sequentially in Colab. Uses OpenCV and scikit-learn for t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f8a35",
   "metadata": {},
   "source": [
    "## 1 — Setup (install packages and imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abf321",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U opencv-python-headless scikit-learn matplotlib --quiet\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "print('OpenCV version:', cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa86169",
   "metadata": {},
   "source": [
    "## 2 — Edge detection (Canny) and Hough Line Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc465059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic image with lines and shapes\n",
    "img = np.zeros((200,300), dtype=np.uint8)\n",
    "cv2.rectangle(img, (20,50), (280,150), 255, -1)\n",
    "cv2.line(img, (0,0), (299,199), 0, 5)\n",
    "\n",
    "edges = cv2.Canny(img, 50, 150)\n",
    "lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=50, minLineLength=30, maxLineGap=10)\n",
    "\n",
    "# visualize\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,3,1); plt.imshow(img, cmap='gray'); plt.title('Original'); plt.axis('off')\n",
    "plt.subplot(1,3,2); plt.imshow(edges, cmap='gray'); plt.title('Canny edges'); plt.axis('off')\n",
    "# draw lines\n",
    "line_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "if lines is not None:\n",
    "    for x1,y1,x2,y2 in lines[:,0]:\n",
    "        cv2.line(line_img, (x1,y1),(x2,y2),(0,0,255),2)\n",
    "plt.subplot(1,3,3); plt.imshow(line_img[:,:,::-1]); plt.title('Hough Lines'); plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49365d9",
   "metadata": {},
   "source": [
    "## 3 — ORB feature detection and descriptor matching (synthetic pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c2e17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two slightly different synthetic images\n",
    "img1 = np.zeros((200,200), dtype=np.uint8)\n",
    "cv2.circle(img1, (100,100), 50, 255, -1)\n",
    "img2 = img1.copy()\n",
    "# add a small translation and rotation to img2\n",
    "M = cv2.getRotationMatrix2D((100,100), 15, 1.0)\n",
    "img2 = cv2.warpAffine(img2, M, (200,200))\n",
    "\n",
    "# ORB detect and compute\n",
    "orb = cv2.ORB_create(nfeatures=200)\n",
    "kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "# BFMatcher with Hamming distance\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1, des2)\n",
    "matches = sorted(matches, key=lambda x: x.distance)[:30]\n",
    "\n",
    "# draw matches\n",
    "match_img = cv2.drawMatches(cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR), kp1, cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR), kp2, matches, None, flags=2)\n",
    "plt.figure(figsize=(10,6)); plt.imshow(match_img[:,:,::-1]); plt.title('ORB Matches'); plt.axis('off'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9495ecac",
   "metadata": {},
   "source": [
    "## 4 — Extract CNN embeddings and visualize with t-SNE (CIFAR-10 subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a03e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a small CNN encoder on CIFAR-10 to get embeddings, then apply t-SNE\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n",
    "# use small subset\n",
    "x = x_train[:2000].astype('float32')/255.0\n",
    "y = y_train[:2000].flatten()\n",
    "\n",
    "# build a small encoder model\n",
    "encoder = models.Sequential([\n",
    "    layers.Input(shape=(32,32,3)),\n",
    "    layers.Conv2D(32,3,activation='relu', padding='same'), layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64,3,activation='relu', padding='same'), layers.MaxPooling2D(),\n",
    "    layers.Flatten(), layers.Dense(128, activation='relu')\n",
    "])\n",
    "embeddings = encoder.predict(x, batch_size=64)\n",
    "print('Embeddings shape:', embeddings.shape)\n",
    "\n",
    "# t-SNE (compute on CPU; may take ~30s for 2000 samples)\n",
    "print('Running t-SNE (this may take a moment)...')\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=42, perplexity=30, n_iter=1000)\n",
    "X2 = tsne.fit_transform(embeddings)\n",
    "\n",
    "# plot with colors\n",
    "plt.figure(figsize=(8,6))\n",
    "for cls in np.unique(y):\n",
    "    idx = np.where(y==cls)\n",
    "    plt.scatter(X2[idx,0], X2[idx,1], s=8, label=str(int(cls)))\n",
    "plt.legend(title='class'); plt.title('t-SNE of CNN embeddings (CIFAR-10 subset)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c3cf9",
   "metadata": {},
   "source": [
    "## 5 — Simple image similarity using embeddings (nearest neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733687ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use embeddings computed above to find nearest neighbors for a sample image\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "dist = euclidean_distances(embeddings)\n",
    "query_idx = 5\n",
    "nn_idx = np.argsort(dist[query_idx])[:6]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "for i, idx in enumerate(nn_idx):\n",
    "    plt.subplot(1,6,i+1)\n",
    "    plt.imshow(x[idx])\n",
    "    plt.title('idx='+str(idx))\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Nearest neighbors in embedding space for query index '+str(query_idx))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32663dec",
   "metadata": {},
   "source": [
    "## 6 — Exercises & Instructor Notes\n",
    "\n",
    "- Replace ORB with SIFT (if OpenCV contrib available) and compare matching performance.\n",
    "- Try UMAP as an alternative to t-SNE for faster embedding visualization (pip install umap-learn).\n",
    "- For large datasets, use FAISS for efficient nearest neighbor search instead of brute-force.\n",
    "- Discuss failures of classical feature matching vs learned descriptors and when to use each approach."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
